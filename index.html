<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Zhuo Zheng - Wuhan University</title>
    <script src="lib/jq214/jquery.js"></script>

    <link rel="stylesheet" type="text/css" href="css/bootstrap-simplex.min.css">
    <script src="lib/bts4b2/bootstrap.bundle.min.js"></script>
    <script src="https://kit.fontawesome.com/0cf3363282.js" crossorigin="anonymous"></script>
    <style>
        #bio {
            text-align: center;
        }

        ul {
            list-style-type: none;
            padding: 0;
        }

        li.link {
            float: left;
            margin-left: 10px
        }

        .myname {
            color: black;
        }

        .itemtitle {
            color: black;
        }

        .my_highlight {
            color: #0028ff
        }

        .paper_highlight {
            color: #9d190b;
        }

    </style>
</head>
<body>


<div class="container" style="margin-top: 20px">
    <div class="row">
        <div class="col-md-3">
            <div id="bio">
                <img src="images/me.jpg" class="rounded-circle" width="150" height="150">
                <p>Zhuo Zheng</p>
                <p><a href="http://www.lmars.whu.edu.cn/">State Key Laboratory of Information Engineering in Surveying,
                    Mapping and Remote Sensing</a></p>
                <p><a href="https://www.whu.edu.cn/">Wuhan University</a></p>
            </div>
        </div>
        <div class="col-md-8">
            <p>
                I am a Ph.D. student (2018-2023, expected) at State Key Laboratory of Information Engineering in
                Surveying,
                Mapping and Remote Sensing (LIESMARS), Wuhan University.
                I received B.S. degree from the School of Geography and Information Engineering, China University of
                Geosciences, Wuhan, China, in 2018.
                I'm now a member of <a href="http://rsidea.whu.edu.cn/">RSIDEA group</a>, advised by Prof. <a
                    href="http://rsidea.whu.edu.cn/">Yanfei Zhong</a> and Prof. <a
                    href="http://www.lmars.whu.edu.cn/prof_web/zhangliangpei/rs/index.html">Liangpei Zhang</a>.
            </p>
            <p>
                My research interest is in remote sensing visual perception and earth vision, especially
                multi-modal and multi-temporal remote sensing image analysis.
                My research goal is to design original and insightful Earth vision technologies to make high positive
                impacts on the geoscience field.
                Meanwhile, I am an enthusiast of remote sensing data science competitions.
            </p>
            <b><span style="color:red;">Update</span>: I am looking for a postdoc research position
                to study Artificial Intelligence for Remote Sensing.
                Feel free to contact me.</b>

            <p>Email: zhengzhuo [at] whu [dot] edu [dot] cn</p>
            <ul>
                <li class="link">
                    <a href="https://github.com/Z-Zheng">
                        <i class="fab fa-github"></i>
                        Github
                    </a>
                </li>
                <li class="link">
                    <a href="https://scholar.google.com/citations?user=CREpn_AAAAAJ&hl=zh-CN">
                        <i class="fab fa-google"></i>
                        Google Scholar
                    </a>
                </li>
                <li class="link">
                    <a href="https://www.researchgate.net/profile/Zhuo_Zheng7">
                        <i class="fab fa-researchgate"></i>
                        ResearchGate
                    </a>
                </li>
                <li class="link">
                    <a href="https://orcid.org/0000-0003-1811-6725">
                        <i class="fab fa-orcid"></i>
                        ORCID
                    </a>
                </li>
                <li class="link">
                    <a href="https://twitter.com/ZhuoZheng2">
                        <i class="fab fa-twitter-square"></i>
                        Twitter
                    </a>
                </li>
            </ul>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12" name="news">
            <h3>Recent News</h3>
            <p>2022.10, Awarded with the 2022 Graduate Academic Innovation Outstanding Prize.</p>
            <p>2021.12, Awarded with "Wang Zhizhuo Innovation Talent" Outstanding Prize.</p>
            <p>2021.10, One paper is accepted by NeurIPS 2021 Datasets and Benchmarks.</p>
            <p>2021.10, One paper is accepted by ISPRS P&RS.</p>
            <p>2021.08, One paper is accepted by RSE.</p>
            <p>2021.07, One paper is accepted by ICCV 2021.</p>
            <p>2021.07, I win the 5th place in the Overhead Geopose Challenge hosted by NGA.</p>
            <p>2021.03, Our team win the 4th place in 2021 IEEE GRSS Data Fusion Contest, Track: Multitemporal Semantic
                Change Detection.</p>
            <p>2021.03, Our <a href="https://www.ingentaconnect.com/content/asprs/pers/2020/00000086/00000003/art00006">PE&RS
                paper</a> wins the first place in the 2021 John I. Davidson President’s Award.
            <p>2020.12, One paper is accepted by ISPRS P&RS.</p>
            <p>2020.11, the <a href="https://github.com/Z-Zheng/FarSeg">source code</a> of FarSeg (CVPR 2020) has been
                available.</p>
            <p>2020.10, Awarded with the 2020 Graduate Academic Innovation Outstanding Prize.</p>
            <p>2020.06, I win the top graduate award at SpaceNet 6 & EarthVision workshop challenge at CVPR 2020.</p>
            <p>2020.05, the <a href="https://github.com/Z-Zheng/FreeNet">source code</a> of FPGA (TGRS 2020) has been
                available.</p>
            <!--            <p>2020.04, One paper is accepted by ISPRS P&RS.</p>-->
            <!--            <p>2020.03, One paper is accepted by TGRS.</p>-->
            <!--            <p>2020.02, One paper is accepted by CVPR 2020.</p>-->
            <!--            &lt;!&ndash;            <p>2020.02, I achieve 4th overall in <a href="https://xview2.org/">xView2 Challenge</a>.</p>&ndash;&gt;-->
            <!--            <p>2020.01, One paper is accepted by TGRS.</p>-->
        </div>
    </div>

    <!--    paper-->
    <div class="row">
        <div class="col-md-12">
            <h3>Selected Publication</h3>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <table class="table table-hover" style="border-collapse: separate">
                <tbody style="font-family: Arial,serif">
                <tr>
                    <td bgcolor="#FFCC99" style="border-style: none; border-width: medium;" valign="top" width="12">&nbsp;</td>
                    <td style="border-style: none; border-width: medium;">
                        <p style="margin-left: 10px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                            <i style="font-size: 15px">
                                ChangeMask: Deep Multi-task Encoder-Transformer-Decoder Architecture for Semantic Change Detection
                            </i>
                            <br>
                            <span style="color: #000000"><b>Zhuo Zheng</b>, Yanfei Zhong, Shiqi Tian, Ailong Ma, Liangpei Zhang</span>
                            <br>
                            ISPRS Journal of Photogrammetry and Remote Sensing (<b>ISPRS P&RS</b>), 2022
                            <br>
                            SCI Q1 Top, ranking it 1 out of 50 in Geography, Physical
                            <br>
                            <span>
                                <a href="https://www.sciencedirect.com/science/article/pii/S0924271621002835">
                                    <span style="color: #808080">paper</span></a>
                                &nbsp;&nbsp;&nbsp;
                                <b>SECOND dataset:
                                    <a href="assets/changemask/SECOND/train.csv"><span style="color: #FF0000">train</span></a> /
                                    <a href="assets/changemask/SECOND/val.csv"><span style="color: #FF0000">val</span></a>
                                    split (ours)
                                </b>
                            </span>
                            <br>
                            <span class="paper_highlight"><b>Highlight:</b> <b class="my_highlight">Disentangled semantic and change representation</b> and <b
                                    class="my_highlight">temporal symmetric transformer (TST)</b> for <b
                                    class="my_highlight">semantic change detection</b>.</span>
                        </p>
                    </td>
                </tr>
                <tr>
                    <td bgcolor="#FFCC99" style="border-style: none; border-width: medium;" valign="top" width="12">&nbsp;</td>
                    <td style="border-style: none; border-width: medium;">
                        <p style="margin-left: 10px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                            <i style="font-size: 15px">
                                Building Damage Assessment for Rapid Disaster Response with a Deep Object-based Semantic Change Detection Framework: from
                                natural disasters to man-made disasters
                            </i>
                            <br>
                            <span style="color: #000000"><b>Zhuo Zheng</b>, Yanfei Zhong, Junjue Wang, Ailong Ma, Liangpei Zhang</span>
                            <br>
                            Remote Sensing of Environment (<b>RSE</b>), 2021
                            <br>
                            SCI Q1 Top, ranking it 1 out of 32 in Remote Sensing
                            <br>
                            <span>
                                <a href="https://www.sciencedirect.com/science/article/pii/S0034425721003564">
                                    <span style="color: #808080">paper</span></a>&nbsp;&nbsp;&nbsp;
                                <b><a href="https://github.com/Z-Zheng/ChangeOS">
                                    <span style="color: #FF0000">code</span>
                                </a></b>
                            </span>
                            <br>
                            <span class="paper_highlight"><b>Highlight:</b> A <b class="my_highlight">deep object-based semantic change
                                    detection</b> method for <b class="my_highlight">building damage assessment</b> in the context of <b
                                    class="my_highlight">disaster response</b>.</span>
                        </p>
                    </td>
                </tr>
                <tr>
                    <td bgcolor="#FFCC99" style="border-style: none; border-width: medium;" valign="top" width="12">&nbsp;</td>
                    <td style="border-style: none; border-width: medium;">
                        <p style="margin-left: 10px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                            <i style="font-size: 15px">
                                Change is Everywhere: Single-Temporal Supervised Object Change Detection in
                                Remote Sensing Imagery
                            </i>
                            <br>
                            <span style="color: #000000"><b>Zhuo Zheng</b>, Ailong Ma, Liangpei Zhang, Yanfei Zhong</span>
                            <br>
                            International Conference on Computer Vision (<b>ICCV</b>), 2021
                            <br>
                            <span>
                                    <a href="https://arxiv.org/abs/2108.07002">
                                        <span style="color: #808080">paper</span>
                                    </a>&nbsp;&nbsp;&nbsp;
                                    <a href="http://zhuozheng.top/changestar">
                                        <span style="color: #808080">project</span>
                                    </a>&nbsp;&nbsp;&nbsp;
                                    <b>
                                        <a href="https://github.com/Z-Zheng/ChangeStar">
                                            <span style="color: #FF0000">code</span>
                                        </a>
                                    </b>
                                </span>
                            <br>
                            <span class="paper_highlight"><b>Highlight:</b> A <b class="my_highlight">single-temporal supervised</b>
                                    learning algorithm and a plug-and-play change detection head: <b
                                        class="my_highlight">ChangeMixin</b> for <b class="my_highlight">change
                                        detection.</b></span>
                            <br>
                            <span class="itemtitle"><b>The method has been included in</b> <a
                                    href="https://github.com/microsoft/torchgeo">microsoft/torchgeo</a></span>
                        </p>
                    </td>
                </tr>
                <tr>
                    <td bgcolor="#FFCC99" style="border-style: none; border-width: medium;" valign="top" width="12">&nbsp;</td>
                    <td style="border-style: none; border-width: medium;">
                        <p style="margin-left: 10px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                            <i style="font-size: 15px">
                                LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation
                            </i>
                            <br>
                            <span style="color: #000000">Junjue Wang<sup>*</sup>, <b>Zhuo Zheng</b><sup>*</sup>, Ailong Ma, Xiaoyan Lu, Yanfei Zhong (* Equal contribution)</span>
                            <br>
                            35th Annual Conference on Neural Information Processing Systems (<b>NeurIPS</b> Datasets & Benchmarks Track), 2021
                            <br>
                            <span>
                                    <a href="https://arxiv.org/abs/2110.08733">
                                        <span style="color: #808080">paper</span>
                                    </a>
                                    &nbsp;&nbsp;&nbsp;
                                    <b>
                                        <a href="https://github.com/Junjue-Wang/LoveDA">
                                            <span style="color: #FF0000">dataset/code</span>
                                        </a>
                                    </b>
                                </span>
                            <br>
                        </p>
                    </td>
                </tr>

                <tr>
                    <td bgcolor="#FFCC99" style="border-style: none; border-width: medium;" valign="top" width="12">&nbsp;</td>
                    <td style="border-style: none; border-width: medium;">
                        <p style="margin-left: 10px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                            <i style="font-size: 15px">
                                Deep Multisensor Learning for Missing-Modality All-Weather Mapping
                            </i>
                            <br>
                            <span style="color: #000000"><b>Zhuo Zheng</b>, Ailong Ma, Liangpei Zhang, Yanfei Zhong</span>
                            <br>
                            ISPRS Journal of Photogrammetry and Remote Sensing (<b>ISPRS P&RS</b>), 2021
                            <br>
                            SCI Q1 Top, ranking it 1 out of 50 in Geography, Physical
                            <br>
                            <span>
                                    <a href="https://www.sciencedirect.com/science/article/pii/S0924271620303476">
                                        <span style="color: #808080">paper</span>
                                    </a>
                                </span>
                            <br>
                            <span class="paper_highlight"><b>Highlight:</b> A <b class="my_highlight">registration-free</b>
                        multi-modal/sensor learning algorithm via exploring <b class="my_highlight">meta-modal/sensory
                            representation</b> for <b class="my_highlight">all-weather mapping</b>.</span>
                        </p>
                    </td>
                </tr>

                <tr>
                    <td bgcolor="#FFCC99" style="border-style: none; border-width: medium;" valign="top" width="12">&nbsp;</td>
                    <td style="border-style: none; border-width: medium;">
                        <p style="margin-left: 10px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                            <i style="font-size: 15px">
                                Foreground-Aware Relation Network for Geospatial Object Segmentation in High
                                Spatial Resolution Remote Sensing Imagery
                            </i>
                            <br>
                            <span style="color: #000000"><b>Zhuo Zheng</b>, Yanfei Zhong, Junjue Wang, Ailong Ma</span>
                            <br>
                            Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020
                            <br>
                            <span>
                                <a href="https://arxiv.org/pdf/2011.09766.pdf">
                                    <span style="color: #808080">paper</span>
                                </a>&nbsp;&nbsp;&nbsp;
                                <b>
                                    <a href="https://github.com/Z-Zheng/FarSeg">
                                        <span style="color: #FF0000">code</span>
                                    </a>
                                </b>
                            </span>
                            <br>
                            <span class="paper_highlight"><b>Highlight:</b> <b class="my_highlight">Explicit foreground modeling</b>
                    from the perspectives of <b class="my_highlight">relation</b> and <b class="my_highlight">optimization</b>
                    for real-time geospatial object segmentation.</span>
                            <br>
                            <span class="itemtitle"><b>The method has been included in</b> <a
                                    href="https://github.com/microsoft/torchgeo">microsoft/torchgeo</a></span>
                        </p>
                    </td>
                </tr>

                <tr>
                    <td bgcolor="#FFCC99" style="border-style: none; border-width: medium;" valign="top" width="12">&nbsp;</td>
                    <td style="border-style: none; border-width: medium;">
                        <p style="margin-left: 10px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                            <i style="font-size: 15px">
                                FPGA: Fast Patch-Free Global Learning Framework for Fully End-to-End
                                Hyperspectral Image Classification
                            </i>
                            <br>
                            <span style="color: #000000"><b>Zhuo Zheng</b>, Yanfei Zhong, Ailong Ma, Liangpei Zhang</span>
                            <br>
                            IEEE Transactions on Geoscience and Remote Sensing (<b>TGRS</b>), 2020
                            <br>
                            SCI Q1 Top
                            <br>
                            <span>
                                <a href="https://ieeexplore.ieee.org/document/9007624">
                                    <span style="color: #808080">paper</span>
                                </a>&nbsp;&nbsp;&nbsp;
                                <b>
                                    <a href="https://github.com/Z-Zheng/FreeNet">
                                        <span style="color: #FF0000">code</span>
                                    </a>
                                </b>
                            </span>
                            <br>
                            <span class="paper_highlight"><b>Highlight:</b> <b class="my_highlight">Patch-free is all you need</b>
                    towards faster and stronger hyperspectral image classification.</span>
                        </p>
                    </td>
                </tr>

                <tr>
                    <td bgcolor="#FFCC99" style="border-style: none; border-width: medium;" valign="top" width="12">&nbsp;</td>
                    <td style="border-style: none; border-width: medium;">
                        <p style="margin-left: 10px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                            <i style="font-size: 15px">
                                HyNet: Hyper-scale object detection network framework for multiple spatial
                                resolution remote sensing imagery
                            </i>
                            <br>
                            <span style="color: #000000"><b>Zhuo Zheng</b>, Yanfei Zhong, Ailong Ma, Xiaobing Han, Ji Zhao, Yanfei Liu,
                Liangpei Zhang</span>
                            <br>
                            ISPRS Journal of Photogrammetry and Remote Sensing (<b>ISPRS P&RS</b>), 2020
                            <br>
                            SCI Q1 Top, ranking it 1 out of 50 in Geography, Physical
                            <br>
                            <span>
                                <a href="https://www.sciencedirect.com/science/article/pii/S0924271620301167">
                                    <span style="color: #808080">paper</span>
                                </a>
                            </span>
                            <br>
                            <span class="paper_highlight"><b>Highlight:</b> <b class="my_highlight">Hyper-scale = Multi-scale &times;
                    Multi-scale</b>, a new perspective of scale modeling at the convolutional groups. </span>
                        </p>
                    </td>
                </tr>

                <tr>
                    <td bgcolor="#FFCC99" style="border-style: none; border-width: medium;" valign="top" width="12">&nbsp;</td>
                    <td style="border-style: none; border-width: medium;">
                        <p style="margin-left: 10px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;">
                            <i style="font-size: 15px">
                                COLOR: Cycling, Offline Learning, and Online Representation Framework for
                                Airport and Airplane Detection Using GF-2 Satellite Images
                            </i>
                            <br>
                            <span style="color: #000000">Yanfei Zhong, <b>Zhuo Zheng<sup>*</sup></b>, Ailong Ma, Xiaoyan Lu, Liangpei Zhang &nbsp (* denotes the corresponding author)</span>
                            <br>
                            IEEE Transactions on Geoscience and Remote Sensing (<b>TGRS</b>), 2020
                            <br>
                            SCI Q1 Top
                            <br>
                            <span>
                                <a href="https://ieeexplore.ieee.org/document/9091107">
                                    <span style="color: #808080">paper</span>
                                </a>
                            </span>
                            <br>
                            <span class="paper_highlight"><b>Highlight:</b> <b class="my_highlight">Evolvable model and dataset</b>:
                    making your model and dataset both great again for new domain data.</span>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
        </div>
    </div>

    <!--    award-->
    <div class="row">
        <div class="col-md-12">
            <h3>Honors and Awards</h3>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <div class="alert alert-dismissible alert-danger">
                <b class="itemtitle">2022 Graduate Academic Innovation Outstanding Prize, Wuhan
                    University&nbsp(武汉大学“研究生学术创新奖”<b class="my_highlight">&nbsp特等奖&nbsp(研究生学术创新校长奖) & “研究生英诺卓越奖学金”</b>)</b><br>
                <a href="https://gs.whu.edu.cn/content.jsp?urltype=news.NewsContentUrl&wbtreeid=1057&wbnewsid=9971">announcement</a>
            </div>
            <div class="alert alert-dismissible alert-danger">
                <b class="itemtitle">2020-2021 "Wang Zhizhuo Innovation Talent" Outstanding Prize &nbsp("王之卓创新人才奖"<b
                        class="my_highlight">&nbsp特等奖</b>)</b><br>
                <a href="http://rsgis.whu.edu.cn/info/1080/9862.htm">announcement</a>
            </div>
            <div class="alert alert-dismissible alert-info">
                <b class="itemtitle">2021 National Scholarship for Graduate Student &nbsp(博士研究生国家奖学金)</b><br>
                <a href="https://www.whu.edu.cn/info/1118/19369.htm">announcement</a>
            </div>
            <div class="alert alert-dismissible alert-warning">
                <b class="itemtitle">Overhead Geopose Challenge, <b class="my_highlight">5th Place (5/444)</b></b><br>
                <small>This challenge is hosted by the National Geospatial-Intelligence Agency (NGA)</small><br>
                <b class="myname">Zhuo Zheng</b> (id: chuchu in the leaderboard)<br>
                <a href="https://www.drivendata.org/competitions/78/overhead-geopose-challenge/">website</a><br>
            </div>
            <div class="alert alert-dismissible alert-warning">
                <b class="itemtitle">2021 IEEE GRSS Data Fusion Contest, Track: Multitemporal Semantic Change Detection,
                    <b
                            class="my_highlight">4th Place</b></b><br>
                <b class="myname">Zhuo Zheng</b>, Junjue Wang, Yinhe Liu, Shiqi Tian, Yanfei Zhong, Ailong Ma<br>
                <a href="https://www.grss-ieee.org/community/technical-committees/2021-ieee-grss-data-fusion-contest-track-msd/">website</a>
                &nbsp
                <a href="https://www.grss-ieee.org/community/technical-committees/2021-ieee-grss-data-fusion-contest-results/">announcement</a>
            </div>
            <div class="alert alert-dismissible alert-info">
                <b class="itemtitle">2021 John I. Davidson President’s Award<b
                        class="my_highlight">, 1st Place</b></b><br>
                <a href="https://www.asprs.org/awards-and-scholarships/award-winners/2021-award-winners.html">announcement</a>
            </div>
            <div class="alert alert-dismissible alert-danger">
                <b class="itemtitle">2020 Graduate Academic Innovation Outstanding Prize, Wuhan
                    University&nbsp(武汉大学“研究生学术创新奖”<b
                            class="my_highlight">&nbsp特等奖&nbsp(研究生学术创新校长奖)</b>)</b><br>
                <a href="https://gs.whu.edu.cn/info/1057/7261.htm">announcement</a>
            </div>
            <div class="alert alert-dismissible alert-warning">
                <b class="itemtitle">SpaceNet 6 & EarthVision workshop challenge at CVPR 2020, <b
                        class="my_highlight">Top Graduate Award</b></b><br>
                <b class="myname">Zhuo Zheng</b>, Junjue Wang, Dingyuan Chen (Team name: __EVER__ in Topcoder)<br>
                <a href="https://spacenet.ai/sn6-challenge/">website</a>
                &nbsp
                <a href="https://medium.com/the-downlinq/spacenet-6-announcing-the-winners-df817712b515">announcement</a>
            </div>
            <div class="alert alert-dismissible alert-warning">
                <b class="itemtitle">xView2 Challenge, <b class="my_highlight">4th Place (4/3500+), overall</b></b><br>
                <b class="myname">Zhuo Zheng</b>, Junjue Wang, Yanfei Zhong, Ailong Ma, Liangpei Zhang<br>
                <a href="https://xview2.org/challenge">website</a>
                &nbsp
                <a href="https://github.com/DIUx-xView/xView2_fourth_place">code</a>
            </div>
            <div class="alert alert-dismissible alert-warning">
                <b class="itemtitle">2019 IEEE GRSS Data Fusion Contest, Single-view Semantic 3D Challenge, <b
                        class="my_highlight">2nd
                    Place</b></b><br>
                <b class="myname">Zhuo Zheng</b>, Yanfei Zhong, Junjue Wang<br>
                <a href="http://www.grss-ieee.org/community/technical-committees/data-fusion/2019-ieee-grss-data-fusion-contest-results/">website</a>
                &nbsp
                <a href="https://ieeexplore.ieee.org/abstract/document/8897927">tech report</a>
            </div>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <h3>Academic Service</h3>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <div class="alert alert-dismissible alert-dark">
                <h5><b>Journal Reviewer</b></h5>
                <ul>
                    <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
                    <li>International Journal of Computer Vision (IJCV)</li>
                    <li>ISPRS Journal of Photogrammetry and Remote Sensing (ISPRS P&RS)</li>
                    <li>IEEE Transactions on Cybernetics (TCYB)</li>
                    <li>IEEE Transactions on Geoscience and Remote Sensing (TGRS)</li>
                    <li>IEEE Geoscience and Remote Sensing Letters (GRSL)</li>
                    <li>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (JSTARS)</li>
                    <li>International Journal of Remote Sensing (IJRS)</li>
                </ul>
                <h5><b>Conference Reviewer</b></h5>
                <ul>
                    <li>ECCV 2022</li>
                    <li>CVPR 2022</li>
                    <li>NeurIPS 2021</li>
                </ul>
                <h5><b>Program Committee</b></h5>
                <ul>
                    <a href="https://www.mair2.com/organizers#h.9nganr5c5yfr">
                        <li>Multi-Agent Interaction and Relational Reasoning Workshop, ICCV 2021</li>
                    </a>
                </ul>
            </div>
        </div>
    </div>


</div>
<div style="text-align: center">
    <a href="https://www.easycounter.com/">
        <img src="https://www.easycounter.com/counter.php?zhuozheng"
             border="0" alt="Web Counter"></a>
    unique visitors,
    <a href="https://www.easycounter.com/">
        <img src="https://www.easycounter.com/counter.php?zhuozhengx"
             border="0" alt="HTML Hit Counter"></a>
    page views since March 2020
    <br>
</div>



</body>

</html>